{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f727d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@satyarepala/understanding-the-confusion-matrix-a-practical-guide-to-validation-metrics-for-binary-classifiers-8062a59613e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f8747d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8f0828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syahidhalid\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Split the data into training and testing sets (50-50 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Calculate the validation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "fpr = fp / (fp + tn)\n",
    "fnr = fn / (fn + tp)\n",
    "ppv = tp / (tp + fp)\n",
    "npv = tn / (tn + fn)\n",
    "mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "informedness = recall + specificity - 1\n",
    "markedness = ppv + npv - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c2cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 90   8]\n",
      " [  7 180]]\n",
      "Accuracy: 0.9473684210526315\n",
      "Precision: 0.9472645201510027\n",
      "Recall: 0.9473684210526315\n",
      "F1 Score: 0.9473036437246962\n",
      "Specificity: 0.9183673469387755\n",
      "False Positive Rate (FPR): 0.08163265306122448\n",
      "False Negative Rate (FNR): 0.0374331550802139\n",
      "Positive Predictive Value (PPV): 0.9574468085106383\n",
      "Negative Predictive Value (NPV): 0.9278350515463918\n",
      "Matthews Correlation Coefficient (MCC): 0.8831053504290323\n",
      "Informedness (Youden's J statistic): 0.8657357679914071\n",
      "Markedness: 0.88528186005703\n"
     ]
    }
   ],
   "source": [
    "# Print the metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"False Positive Rate (FPR):\", fpr)\n",
    "print(\"False Negative Rate (FNR):\", fnr)\n",
    "print(\"Positive Predictive Value (PPV):\", ppv)\n",
    "print(\"Negative Predictive Value (NPV):\", npv)\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
    "print(\"Informedness (Youden's J statistic):\", informedness)\n",
    "print(\"Markedness:\", markedness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9ac5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion:\n",
    "\n",
    "#Confusion matrices and validation metrics play a crucial role in evaluating the performance of binary classifiers. They provide valuable insights into the classifier’s ability to make correct predictions and its behavior in different classes. In this discussion, we explored the fundamental components of the confusion matrix and the interpretation of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "#We learned about essential validation metrics derived from the confusion matrix, such as accuracy, precision, recall (sensitivity), specificity, and the F1 Score. These metrics offer different perspectives on classifier performance and help us understand its strengths and weaknesses in various scenarios.\n",
    "\n",
    "#Moreover, we delved into additional metrics that further enrich the evaluation process, especially when dealing with class imbalance. Metrics like the False Positive Rate (FPR), False Negative Rate (FNR), Positive Predictive Value (PPV), Negative Predictive Value (NPV), Matthews Correlation Coefficient (MCC), Informedness (Youden’s J statistic), and Markedness allow for a more comprehensive assessment of the classifier’s performance, particularly in scenarios where one class significantly outnumbers the other.\n",
    "\n",
    "#The choice of appropriate evaluation metrics is crucial and depends on the specific problem domain, the distribution of classes, and the objectives of the binary classification task. For instance, when handling class imbalance, metrics that balance precision and recall, such as the F1 Score and MCC, prove to be more informative.\n",
    "\n",
    "#In conclusion, confusion matrices and validation metrics serve as indispensable tools in understanding and optimizing binary classifiers. By leveraging these metrics effectively, we can make informed decisions about the model’s performance, identify areas of improvement, and tailor the classifier to meet the specific requirements of the problem at hand. With a solid understanding of these concepts, data scientists and machine learning practitioners can confidently navigate the world of binary classification and make meaningful contributions to real-world applications ranging from medical diagnosis and fraud detection to image recognition and customer churn prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
